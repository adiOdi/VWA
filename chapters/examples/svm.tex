\section{Support vector machines}
\label{sec:SVM}
\subsection{K-nearest-Neighbours}
As a parameterless approach, KNN works by saving the whole of the training data, representing it in a n-dimensional space, and letting the k-nearest neighbors decide how to act. There are only few parameters to control now, and the amount of parameters is no longer strictly bound by the complexity of the problem at hand. Mainly, there remain: the k, the weight of the different dimensions, and the way the neighbors are combined. 
The weight depends on the real-live weight of the dimensions (e.g. color could have a lower significance than the size of an object to categorize).
As for the ways in which the neighbors are combined: There are many options here, like e.g. Majority vote, average, weighed average, ...
\subsection{SVMs}
A problem with KNN type approaches is high memory usage, as all the training data has to be stored. To combat this, one can use the fact that in most problems the entries close to the border are more helpful than entries in the middle of a decisive cluster.

So as an extension of KNNs, SVMs were developed. They come with most of the strengths of KNNs, while eliminating some weaknesses.

In a SVM only the few entries along the border are stored, these are called support vectors, as they "hold up" the border. When a decision needs to be made, only the side of the border has to be checked to come to a conclusion. This can also mean a significant increase in efficiency, as not all distances have to be checked like in KNN.

This reduces the problem to one of fitting a curve to some points, which is already well-researched, and the parameters to choose here are on how detailed the curve can be while still not overfitting.
\subsection{Usage}
SVMs are often the first approach tried, as they are relatively simple, and yield good results in a wide variety of use cases. 

They are relatively simple to view and understand, should there only be few important dimensions.