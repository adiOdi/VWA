\section{Decision trees}
\label{sec:decisionTrees}
\subsection{Definition}

A decision tree is a tree consisting of nodes, in which each node answers a yes/no question. 

A decision tree can be made symbolically, but to create one subsymbolically, data is needed, which is used to shape the tree accordingly.
The hard part is figuring out what question the next node will split the data by. 

One method of measuring the usefulness of a question is the Gini method.
It is calculated as follows: Gini impurity $= 1-(yes/n)^2-(no/n)^2$
The smaller this impurity, the less mixed the output is. Which is what we want, as less mixed means the tree is surer of its output. 
So to choose the next node to add to a tree, we measure the GINI impurity of all possible parameters, and choose the one with the smallest impurity. We repeat this process until we only have few samples left in each leaf. 
%example:
$https://www.w3schools.com/python/python_ml_decision_tree.asp$

Once created, to find an answer, one must walk down the tree, following the path the nodes lead you, until an end (a leaf) is met.

\subsection{Usage}
There are two advantages of decision trees in comparison to other forms of AI. One is the ease of understanding it, as we can retrace decisions with relative ease, which is very important in applications where trust in an algorithm is an issue. So for example in an environment where job applications or prison sentences are guided by algorithms, it might be required to be able to check the algorithm for, for example, discriminatory behavior.

Another benefit decision trees offer is the low computational complexity, in both training and application. Of course, they can grow arbitrarily large, but with the right techniques they can be brought back to reasonable sizes, which just capture the essentials.

There are two methods for cutting the size of a tree: Early stopping and pruning. In early stopping we stop a branch as soon as we cannot split the data with sufficiently low Gini impurity. This can be very efficient, but there are cases where one question does not help at all, but two do. One such case would be a XOR-Gate, as one input alone does not provide enough information, so the tree would stop, although the question could be answered by two questions.
So to avoid stopping too early, one can first generate the whole tree, and then prune useless branches away.

Of course, decision trees are not perfect: For one they require a lot of data to be conclusive, and as they are one of the simpler forms of AI, they need to get quite big to capture complex concepts.

\subsection{Categorization}
Decision trees can be categorized as subsymbolical (more symbolical than other algorithms through), are trained supervised, and are a sort of parameterless algorithm.

\myfigure{figures/VWA-tree.pdf}
    {width=0.8\textwidth, height=0.5\textheight} % max width / height
    {This Image depicts a decision tree}   % caption
    {decision tree}   % optional short caption for table of figures
    {fig:decisiontree}    % label

 