\section{Q-Tables}
\label{sec:QTable}
A Q-Table is a very literal interpretation of the definition of AI: As an AI maps inputs to outputs, a very simple method would just be a table. To implement such a table is only possible in discrete space, as every state has to have an action assigned to it.

To train a Q-Table is relatively straightforward: There is a score assigned to every State-Action pair \autoref{fig:qtable}. Then the agent can learn with reinforcement learning: Should an action lead to a positive reward the score of the previous state-action pair is updated up slightly and vice versa.

\myfigure{figures/Qtable.pdf}
    {width=0.8\textwidth, height=0.5\textheight} % max width / height
    {Q-Table}   % caption
    {Q-Table}   % optional short caption for table of figures
    {fig:qtable}    % label

To update the values often the function $Q_{new}=(1-\alpha)*Q_{old}+\alpha*reward$ is used.($\alpha=$ learning rate)
In the example \autoref{fig:qtable} this would mean: when the first entry (1|1|5 => 0.5|0.2) is found by the agent, it would probably try moving left. When the reward is 1 for this action, it would update the entry as follows: $\alpha=0.1$\\$Q_{new}=(1-0.1)*0.5+0.1*1=0.55$, so the entry would now be: 1|1|5 => 0.55|0.2

