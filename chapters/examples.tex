\chapter{Examples}
% Q-table %(flappy bird)

% decision tree %(https://www.w3schools.com/python/python_ml_decision_tree.asp)

% neural net back propagation 

% Evolution

\section{Decision trees}
\subsection{Definition}

A decision tree is a tree consisting of nodes, in which each node answers a yes/no question. 

A decision tree can also be made symbolically, but to create such a tree  subsymbolically, data is needed, which is used to shape the tree accordingly.
The hard part is figuring out what question the next node will use for its split. 

One method of measuring the usefulness of a question is the Gini method.
It is calculated as follows: Gini impurity $= 1-(yes/n)^2-(no/n)^2$
The smaller this impurity, the less mixed the output is. Which is what we want, as less mixed means the tree is surer of its output. 
So to choose the next node to add to a tree, we measure the GINI impurity of all possible parameters, and choose the one with the smallest impurity. We repeat this process until we only have few samples left in each leaf. %example

Once created, to find an answer, one must walk down the tree, following the path the nodes lead you, until an end (a leaf) is met.

\subsection{Usage}
There are two advantages of decision trees in comparison to other forms of AI. One is the ease of understanding it, as we can retrace decisions with relative ease, which is very important in applications where trust in an algorithm is an issue. So for example in an environment where job applications or prison sentences are guided by algorithms, it might be required to be able to check the algorithm for, for example, discriminatory behavior.

Another benefit decision trees offer is the low computational complexity, in both training and application. Of course, they can grow arbitrarily large, but with the right techniques % pruning
they can be brought back to reasonable sizes, which just capture the essentials.

There are two methods for cutting the size of a tree: Early stopping and pruning. In early stopping we stop a branch as soon as we cannot split the data with sufficiently low Gini impurity. This can be very efficient, but there are cases where one question does not help, but two do. 
So to avoid stopping too early, one can first generate the whole tree, and then prune useless branches away.

Of course, they are not perfect: For one they require a lot of data to be conclusive, and as they are one of the simpler forms of AI, they need to get quite big to capture complex concepts.

\section{Neural nets}
\subsection{Definition}
A neural net is a structure where values are passed on through layers of nodes. Each node performs only a relatively simple operation with parameters inherit to each specific node.
%image

First the amount of layers and nodes has to be defined, and to train the neural net, these parameters can be tuned.

As there are normally too many possible values for the parameters not every possible combination can be brute forced. There are different methods for tuning the parameters. Two examples are back propagation and evolutionary learning. Of course these parameters could, in theory, also be filled in by hand, but this is normally not feasible.

In backpropagation, a supervised method, an input is given, and the output is compared to the known correct output. Now the nodes responsible for the biggest error are slightly nudged in the right direction.

\section{Comparison}
% complexity
% accuracy
% training
% application
% performance
% resources